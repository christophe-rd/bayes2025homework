---
title: "Homework 2"
output: html_document
date: "2025-01-27"
---

```{r setup, include=FALSE}
wd <- '~/projects/ubc_related/courses/bayes2025homework/homeworksubmitted/victor/hmw2'

library(hypervolume)
library(doFuture)
library(dplyr)
library(gpkg)
library(terra)
library(rstan)
library(ggplot2)

run_hypervolume <- FALSE
process_gpkg <- FALSE
process_newdata <- FALSE

```

```{r data}

# load input data
bodymass <- read.csv(file.path(wd, 'input', 'carnivorebodymass.csv'))
teeth <- read.csv(file.path(wd, 'input', 'carnivoreteeth.csv'))

# load supplementary tables of the original paper
table1 <- read.csv(file.path(wd, 'input', 'table_1.csv'))
colnames(table1) <- c('species_A', 'species_B', 'family', 'time', 
                      'rangesize_A', 'rangesize_B', 'overlap', 'mass_A', 'mass_B')
table2 <- read.csv(file.path(wd, 'input', 'table_2.csv'))
colnames(table2) <- c('species_A', 'species_B', 'M1_A', 'M1_B', 
                      'PM4_A', 'PM4_B', 'CsupL_A', 'CsupL_B')
data_supp <- inner_join(table1, table2, by = c('species_A', 'species_B'))

```

## Data

### First exploration

```{r metrics, results='hide'}

# a first idea is to compute an index of functionnal diversity, based on the tooth length
# this can be done by computing hypervolume similarity
teeth_filtered <- teeth %>%
  dplyr::select('Species', 'PM4', 'CsupL') %>%
  na.omit() %>%
  group_by(Species) %>%
  mutate(nobs = n()) %>%
  filter(log(nobs) >= 2) # min number of obs. to compute a 2D hypervolume 

# compute a 2-dimensional functional hypervolume per species
if(run_hypervolume){
  tb <- Sys.time()
  plan(multisession, workers = 18)
  y <- foreach(s = unique(teeth_filtered$Species)) %dofuture% {
    fun_hpv <- hypervolume(teeth_filtered[teeth_filtered$Species == s, c('PM4', 'CsupL')], method='gaussian')
    saveRDS(fun_hpv, file = file.path(wd, 'output/trait_hypervolume', paste0(s, '.rds')))
  }
  plan(sequential);gc()
  te <- Sys.time()
  print(te-tb) # less than 2 minutes on my computer
}

# for each species pair in the original paper, compute the similarity between the hypervolumes
data_supp <- data_supp %>%
  filter(species_A %in% teeth_filtered$Species & species_B %in% teeth_filtered$Species)
data_supp$fun_dissimilarity <- NA
for(i in 1:nrow(data_supp)){
  hpv_A <- readRDS(file.path(wd, 'output/trait_hypervolume', paste0(data_supp[i, 'species_A'], '.rds')))
  hpv_B <- readRDS(file.path(wd, 'output/trait_hypervolume', paste0(data_supp[i, 'species_B'], '.rds')))
  
  hpv_set <- hypervolume_set(hpv_A, hpv_B, check.memory=FALSE)
  hpv_stats <- hypervolume_overlap_statistics(hpv_set)
  
  data_supp[i, 'fun_dissimilarity'] <- 1-hpv_stats['sorensen']
}
plot(data_supp$overlap ~ data_supp$fun_dissimilarity)
# unfortunately, we don't have a lot of data

```

### More species

```{r ranges, results='hide'}

# let's try with more species! not only the closest pairs of the paper
# we thus need to compute the range overlap for these species combinations
if(process_gpkg){
  # data come from the Mammal Diversity Databse
  g <-  geopackage('/home/victor/Downloads/MDD/MDD_Carnivora.gpkg', connect = TRUE)
  mdd <- gpkg_vect(g, 'MDD_Carnivora')
  mdd_select <- mdd[mdd$sciname %in% unique(teeth_filtered$Species)]
  writeVector(mdd_select, filename =file.path(wd, 'output', 'carnivor_ranges.shp'))
  RSQLite::dbDisconnect(g) 
  rm(g, mdd, mdd_select)
  gc()
}else{
  mdd_ranges <- vect(file.path(wd, 'output', 'carnivor_ranges.shp'))
}

```

```{r newdata, results='hide'}

if(process_newdata){
  
  # unique pairs of species
  species_comb <- t(combn(unique(teeth_filtered$Species), 2))
  
  # create our own dataframe!
  newdata <- data.frame(species_A = NA, species_B = NA, 
                        range_A = NA, range_B = NA,
                        range_overlap_area = NA, fun_similarity = NA)
  # .mdd_ranges <- wrap(mdd_ranges) # needed for parallel computation
  # plan(multisession, workers = 2)
  for(i in 1:nrow(species_comb)){
    cat(paste0(i, '\n'))
    #mdd_ranges <- rast(.mdd_ranges)
    # compute range metrics
    range1 <-  mdd_ranges[mdd_ranges$sciname %in% species_comb[i,1]]
    range2 <-  mdd_ranges[mdd_ranges$sciname %in% species_comb[i,2]]
    int12 <- terra::intersect(range1, range2)
    newdata[i, 'species_A'] <- species_comb[i,1]
    newdata[i, 'species_B'] <- species_comb[i,2]
    range_A <- expanse(range1, unit = 'km')
    newdata[i, 'range_A'] <- ifelse(length(range_A) == 0, NA, range_A)
    range_B <- expanse(range2, unit = 'km')
    newdata[i, 'range_B'] <- ifelse(length(range_B) == 0, NA, range_B)
    int_area <- expanse(int12, unit = 'km')
    newdata[i, 'range_overlap_area'] <- ifelse(length(int_area) == 0, 0, int_area)
    rm(range1, range2, int12);gc()
    
    # compute functional dissimilarity metrics
    hpv1 <- readRDS(file.path(wd, 'output/trait_hypervolume', paste0(species_comb[i,1], '.rds')))
    hpv2 <- readRDS(file.path(wd, 'output/trait_hypervolume', paste0(species_comb[i,2], '.rds')))
    hpv_set <- hypervolume_set(hpv1, hpv2, check.memory=FALSE)
    hpv_stats <- hypervolume_overlap_statistics(hpv_set)
    newdata[i, 'fun_similarity'] <- hpv_stats['sorensen']
  }
  saveRDS(newdata, file.path(wd, 'output', 'newdata.rds'))
  
}else{
  
  newdata <- readRDS(file.path(wd, 'output', 'newdata.rds'))
  
}

```

```{r viz_newdata, results='hide'}

newdata <- na.omit(newdata)
newdata$spat_similarity <- 2*newdata$range_overlap_area/(newdata$range_A+newdata$range_B) # similar to hypervolume similarity (Sorensen index)
newdata <- newdata %>% 
  rowwise() %>% 
  mutate(range_overlap_jdavies = range_overlap_area/max(c(range_A, range_B)), # the way J. Davies computed his range overlap metrics
         fun_dissimilarity = 1-fun_similarity)
plot(newdata$fun_dissimilarity ~ (newdata$spat_similarity))
# this looks bad, but it will be a chance to have fun with Stan! (and justify the time I spent gathering the data...)
```

## Workflow

### Step 1:  Develop the model

```{stan, output.var = "dunno", eval = FALSE}

// inspired by https://betanalpha.github.io/assets/case_studies/mixture_models.html


data{
  
  int n;
  real<lower=0, upper=1> y[n];
  real<lower=0, upper=1> x[n];
  
}

parameters{
  
  real<lower=0, upper=1> a;
  real<lower=0> d;
  real<lower=0, upper=1> m;
  real<lower=0> phi;
  simplex[2] lambda;
}

model{
  
  vector[n] mu;
  row_vector[2] shape ;
  
  a ~ normal(0.5,0.25);
  d ~ normal(10,5);
  m ~ normal(0.5,0.25);
  phi ~ normal(30,10);
  lambda ~ dirichlet(rep_vector(1,3));
  
  for(i in 1:n){
    
    // we inflate on ones (we don't need it for zeros)
    f(y[i] == 1){
      target += log(lambda[1]);
    } else {
      mu[i] = a/(1+exp(d*(m-x[i])/a));
      shape = [mu[i] * phi, (1 - mu[i] ) * phi];
      target += log(lambda[2]) + beta_lpdf(y[i] | shape[1], shape[2]);
    } 
    
  }
  
}


```

### Step 2: Simulate data, fit the model on it

```{r simulate, results='hide'}

# non-ones
logistic <- function(x, par){
  
  return(par[1]/(1+exp(par[2]*(par[3]-x)/par[1])))
  
}
A <- 0.9 # asymptote
D <- 15 # max derivative
M <- 0.05 # location of the sigmoid
phi <- 20
# create some non-zeros
xnz <- runif(100)
yhat <- sapply(xnz , logistic, par = c(A,D,M))
y <- rbeta(100, shape1 = yhat*phi, shape2 = (1-yhat)*phi)
plot(y ~ xnz )
points(yhat ~ xnz , col = "darkblue")
non_zeros <- data.frame(x = xnz ,y)

# some zeros x data
y <- runif(50)
zeros_x <- data.frame(x = 0 ,y)

# some ones y data
x <- runif(50)
ones_y <- data.frame(x = x ,y = 1)

datasim <- rbind(non_zeros, zeros_x, ones_y)
plot(datasim$y~ (datasim$x))

```

```{r simulate.fit, results='hide'}

# let's run the model

mdl.data <- list(y = datasim$y,
                 n = nrow(datasim),
                 x = datasim$x)

fit <- stan(file.path(wd, "beta_oneinflated.stan"), 
            data = mdl.data,
            iter = 4000,
            warmup = 3000,
            chains = 4,
            cores = 4)

fit.summ <- summary(fit)$summary

c(fit.summ["a","mean"], fit.summ["d","mean"], fit.summ["m","mean"], fit.summ["phi","mean"])
c(A,D,M,phi)
# not super great!

# but the shape of the curve is not so bad (red = simulated, blue = yhat)
ysim <- sapply(x, logistic, par = c(fit.summ["a","mean"], fit.summ["d","mean"], fit.summ["m","mean"]))
plot(ysim ~ x, xlim=c(0, 1), ylim=c(0, 1), col = "darkred")
points(yhat ~ xnz, col = "darkblue")


```

### Step 3: Run the model on the empirical data

```{r empirical.fit, results='hide'}

# let's run the model

mdl.data <- list(y = newdata$fun_dissimilarity,
                 n = nrow(newdata),
                 x = newdata$spat_similarity)

fit <- stan(file.path(wd, "beta_oneinflated.stan"), 
            data = mdl.data,
            iter = 4000,
            warmup = 3000,
            chains = 1,
            cores = 1)

fit.summ <- summary(fit)$summary

c(fit.summ["a","mean"], fit.summ["d","mean"], fit.summ["m","mean"], fit.summ["phi","mean"])

# but the shape of the curve is not so bad (red = simulated, blue = yhat)
ysim <- sapply(newdata$spat_similarity, logistic, par = c(fit.summ["a","mean"], fit.summ["d","mean"], fit.summ["m","mean"]))
plot(newdata$fun_dissimilarity ~ newdata$spat_similarity, xlim=c(0, 1), ylim=c(0, 1), col = "grey",
      ylab="Functional dissimilarity", xlab="Range similarity")
points(ysim ~ newdata$spat_similarity, col = "darkred")

samples <- rstan::extract(fit)
simulations <- data.frame(
  y = as.numeric(samples$ysim),
  x = as.numeric(samples$xsim)
)

ggplot() +
  geom_point(aes(y = y, x = x), data = simulations, col = "grey80") +
  stat_summary(geom = "line", fun.y = "mean", data = simulations %>% filter(y != 1), aes(y = y, x = round(x,2)), col = "white", size = 2) +
  stat_summary(geom = "line", fun.y = "mean", data = simulations %>% filter(y != 1), aes(y = y, x = round(x,2)), col = "darkred") +
  geom_boxplot(aes(y = 0.98, x = x), data = simulations %>% filter(y == 1), width = 0.02, col = "darkred")+
  theme_bw() +
  coord_cartesian(expand = FALSE)
# boxplot represent the fun.diss=1 observations
# curve represent the fun.diss!=1 mean simulations

par(mfrow = c(1,2))
hist(newdata$fun_dissimilarity, main = "Observed fun. dissimilarity")
hist(simulations$y, main = "Simulated fun. dissimilarity")

  

```
